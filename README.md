# Comment Toxicity Detection 

## Summary

This project aims to build a machine learning model that can classify the toxicity level of comments into six categories:

- toxic
- severe toxic
- obscene
- threat
- insult
- identity hate

The model uses a Sequential deep learning architecture with an Embedding layer, Bidirectional LSTM layer, and fully connected layers to achieve the task.


## Real-World Applications

The Comment Toxicity Detection Project has numerous real-world applications across various domains:

**Social Media Moderation:** Social media platforms can utilize this comment toxicity detection model to automatically flag and moderate harmful, offensive, or abusive comments, creating a safer and more positive online community for users.

**Online Forums and Discussion Boards:** Forums and discussion boards can integrate this comment toxicity detection to prevent the spread of hate speech, cyberbullying, and inappropriate content, fostering a healthier and more inclusive online environment for users. It enables constructive discussions and discourages toxic behavior.

## Demo
<img width="1087" alt="Screenshot 2023-07-17 at 11 11 18 PM" src="https://github.com/mukunth26/Toxic_Comment_detection/assets/130204205/4452b7b2-3f21-4007-8f2e-670315325202">



